<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

    <title>Unquarked</title>
    <generator uri="https://github.com/jekyll/jekyll">Jekyll v2.4.0</generator>
		<icon>/apple-touch-icon-precomposed.png</icon>
    <subtitle>A place for the ramblings of Dale</subtitle>
    <link href="/atom.xml" rel="self"/>
    <link href="/" rel="alternate" type="text/html"/>
    <updated>2014-12-22T11:30:55-05:00</updated>
    <id>/</id>
    <author>
			<name></name>
			<uri>/</uri>
			
		</author>

    
    <entry>
        <title>A Simple Wristwatch</title>
        <link href="/a-simple-wristwatch/"/>
        <updated>2014-11-28T01:57:01-05:00</updated>
        <id>/a-simple-wristwatch</id>
        <author>
					<name></name>
					<uri>/</uri>
					
				</author>
        <content type="html">
        	
        	&lt;p&gt;In an attempt to master Eagle and surface-mount soldering, I decided to
design a simple LED wristwatch with as small a footprint as I could manage.
As you can see in this &lt;a href=&quot;/files/wristwatch.sch&quot;&gt;schematic&lt;/a&gt; (board files
  &lt;a href=&quot;/files/wristwatch1.brd&quot;&gt;here&lt;/a&gt;), the
watch runs on an ATMega328P.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/wristwatchschematic.png&quot; alt=&quot;schematic&quot; /&gt;&lt;/p&gt;

&lt;p&gt;I chose this chip because it’s the same chip
in an Arduino Uno, and there are many tutorials online for how to upload
code to it via an &lt;a href=&quot;http://arduino.cc/en/Tutorial/ArduinoISP&quot;&gt;Arduino ISP&lt;/a&gt;. I
also decided to use a real time clock with a built-in crystal to keep time. This
RTC is pretty expensive ($8 on mouser?), but keeps time extremely accurately. On
V2, I might use a cheaper RTC or forgo the RTC and add a 16Mhz crystal instead.
At the present, the Arduino is designed to use its own internal 8Hz oscillator as a clock.&lt;/p&gt;

&lt;p&gt;I meticulously reviewed the schematics and printed 3 boards for around $10 from
OSHPark.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/wristwatchboards.jpg&quot; alt=&quot;oshpark&quot; /&gt;
Nonetheless, I found after soldering 90% of the components that &lt;strong&gt;the schematic is broken&lt;/strong&gt;!
There’s a whole row of LED’s that are connected to nothing,
and there’s no easy way to get at the clock pin. I tried bootloading the thing
with an Arduino, but haven’t had success yet.  News to come with V2.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/bootloading.jpg&quot; alt=&quot;bootloading&quot; /&gt;&lt;/p&gt;

        </content>
    </entry>
    
    <entry>
        <title>Neuromancer</title>
        <link href="/neuromancer/"/>
        <updated>2014-11-28T00:52:18-05:00</updated>
        <id>/neuromancer</id>
        <author>
					<name></name>
					<uri>/</uri>
					
				</author>
        <content type="html">
        	&lt;img src=&quot;/images/joe_eeg_feature.jpg&quot;&gt;&lt;br/&gt;
        	&lt;p&gt;Our brains often misbehave; they make us bored, anxious, procrastinatory,
they get distracted at all the wrong times. Many of us accept this behavior as
an unchangeable part of life, and why not? Thought is a difficult process to
debug, given what limited access we have to our true internal mental state. What
we need to overcome our mental bad habits, then, is a sort of debugger; a device
that tells us more than what we can intuit given our own stream of consciousness.&lt;/p&gt;

&lt;p&gt;Neuromancer is such a device for helping us improve our concentration. Comprised
of an EEG headset (either an &lt;a href=&quot;www.emotiv.com&quot;&gt;Emotiv&lt;/a&gt; or BioSemi EEG) and a software
suite, it aims to improve our focus by giving us real-time neurofeedback on our
current state of concentration. The Neuromancer package, whose (alpha) source can be found
&lt;a href=&quot;https://github.com/dalequark/emotivExperiment&quot;&gt;here&lt;/a&gt;, consists of a Java backend,
browser frontend, and a handful of Python scripts for data processing.  Neuromancer
provides users with a concentration-based task while simultaneously measuring neural
signals via EEG. After training a classifier that determines the relative face/place
focus of a user, Neuromancer gives users real time neurofeedback on their focus
by altering the presented stimulus.  fMRI studies have shown that such an experiment
can improve participants’ performance on concentration tasks, even after the
neurofeedback is removed, as compared to controls.  &lt;/p&gt;

&lt;p&gt;More details to come soon. Stay tuned!&lt;/p&gt;

        </content>
    </entry>
    
    <entry>
        <title>CamSense: An Emotion-Controlled Camera</title>
        <link href="/camsense-an-emotion-controlled-camera/"/>
        <updated>2014-11-26T17:06:39-05:00</updated>
        <id>/camsense-an-emotion-controlled-camera</id>
        <author>
					<name></name>
					<uri>/</uri>
					
				</author>
        <content type="html">
        	&lt;img src=&quot;/images/camsense_feature.jpg&quot;&gt;&lt;br/&gt;
        	&lt;p&gt;For Fall HackPrinceton ‘14, Joe, Lucas and I made a camera that was triggered
via spikes in electrodermal response. It worked super well! See the demo below or check
out the
&lt;a href=&quot;http://www.instructables.com/id/CamSense-An-Emotion-Triggered-Camera/&quot;&gt;instructables&lt;/a&gt;.&lt;/p&gt;

&lt;iframe width=&quot;420&quot; height=&quot;315&quot; src=&quot;//www.youtube.com/embed/Y5tTZEzH844&quot; frameborder=&quot;0&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

        </content>
    </entry>
    
    <entry>
        <title>Adventures in DIY EEG Land</title>
        <link href="/DIY-EEG/"/>
        <updated>2014-09-30T00:00:00-04:00</updated>
        <id>/DIY-EEG</id>
        <author>
					<name></name>
					<uri>/</uri>
					
				</author>
        <content type="html">
        	&lt;img src=&quot;/images/dalebrainlarge.png&quot;&gt;&lt;br/&gt;
        	&lt;p&gt;I always thought that mind reading was pretty amazing, and volunteered for
all sorts of campus psych studies involving EEG and fMRI. That’s how I got
this lovely picture of my brain, as well as the same picture in 3D.  At the beginning of the summer, though, I learned that was (no pun) mind-blowing.  It’s possible to mind-read for less than $20.  So began my journey into DIY EEG tech.  &lt;/p&gt;

&lt;p&gt;Our thoughts stir a whirlwind of electrical activity along our scalps.  Engagement, meditation, sleep, and even the subjects of our thoughts create faint but real electrical patterns that we can measure non-intrusively by placing electrodes on our scalps.  Electroencephalographs or EEGs do just this, creating a timeseries of voltage levels distributed in space along the head.  Sometimes, our thoughts manifest themselves as distinct frequencies in these voltages. For example, sleep, meditation, and concentration often have distinct frequency signals.  Other times, the mappings between EEG and our thoughts are too complicated for us to discern simply by frequency analysis, and we must apply machine learning techniques to map EEG data to states of mind.  The biggest problem neuroscientists face in interpreting EEG data is not hwo to interpret it, though, but how to tease brain data out from noisy signals.  In the past, it would have been impossible for a hobbyist to make a quality EEG simply because a DIY EEG would be cursed with a terrible signal-to-noise ratio, from noise in 60Hz frequencies of power lines (in the US), from eyeblink movements and facial expressions.  Today, this is still the primary problem a hobbyist faces, but hardware and software have come together to improve SNR’s.  Since June, I’ve investigated in depth EEG options availible to hobbyists, and this is my DIY EEG review.&lt;/p&gt;

&lt;h1 id=&quot;diy-eegs&quot;&gt;DIY EEG’s&lt;/h1&gt;
&lt;p&gt;## A Cheap, Single Channel EEG&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/eeg_circuit.jpeg&quot; alt=&quot;Single Channel EEG&quot; /&gt;
  My EEG adventures began with an EEG for measuring only a single channel of eeg data.  It’s pretty easy to measure only a single channel accurately. You can do it for under $30 with an Arduino and some amplifiers.  The breadboarded circuit above is a picture of my single channel EEG, which I made under the guidance of very helpful &lt;a href=&quot;https://sites.google.com/site/chipstein/home-page/eeg-with-an-arduino&quot;&gt;Chipstein&lt;/a&gt;’s instructions.  Many current, cheap commercial devices have only a single channel of data (&lt;a href=&quot;http://www.thinkmelon.com/&quot;&gt;Melon&lt;/a&gt;, &lt;a href=&quot;http://store.neurosky.com/products/mindwave-1&quot;&gt;Mindwave&lt;/a&gt;, and Mindflex to name a few). I didn’t analyze this data in depth, but apparently it can provide a general sense of focus/engagement if interpreted correctly. I’m not too skeptical of this, since even &lt;a href=&quot;/camsense-an-emotion-controlled-camera/&quot;&gt;skin conductivity&lt;/a&gt; can tell us a lot about our emotional levels.  &lt;/p&gt;

&lt;h2 id=&quot;eegmouse-and-openbci-high-quality-8-channel-eeg&quot;&gt;EEGMouse and OpenBCI: High Quality, 8 Channel EEG&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;http://openelectronicslab.github.io/eeg-mouse/&quot;&gt;EEGMouse&lt;/a&gt; is a super cool, open source project developed by some very helpful hackers in the Netherlands. Their board files, which are availible online, essentially break out the “TI ADS1299 8-Channel 24-Bit analog front-end” so that it can interface with an Arduino. TI has just created an awesome line of chips that are designed to process EEG, EKG, and EMG signals, which include amplifiers and filters for many channels in a single chip. These chips, like the TI ADS1299, already appear in several consumer products, but EEGMouse and &lt;a href=&quot;openbci.com&quot;&gt;OpenBCI&lt;/a&gt; are of the first open source projects I’ve seen to use them.  As a result, if used with high quality electrodes and conductive paste, these devices can produce very impressive signals for a hobbyist device.   These devices are very similar (OpenBCI also uses a chip from the TI ADS family), the main difference being that OpenBCI was a Kickstarted project that sells their prefabricated boards at ~$450, whilst EEGMouse is an open source project that provides board files, Arduino code, and instructions, but no physical product.  Thus to use EEGMouse, hackers have to have their own boards fabricated.  I was lucky enough to snag one of the EEGMouse developers’ unused V1 boards, but soldering on the tiny ADS1299 chip proved to be challenging:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/eegboard.jpg&quot; alt=&quot;eegmouseboard&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The chip, by the way, is pretty pricy unless you’re a student who has access to TI’s free chip sample program. So, the EEGMouse is a cheaper EEG than those from OpenBCI, but it’s not for the faint of [soldering] heart.&lt;/p&gt;

&lt;h1 id=&quot;consumer-eegs&quot;&gt;Consumer EEG’s&lt;/h1&gt;

&lt;h2 id=&quot;emotiv-epoc&quot;&gt;Emotiv Epoc&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;/images/dale_emotiv.jpg&quot; alt=&quot;Me with my Emotiv Epoc&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In my research, I use an &lt;a href=&quot;https://emotiv.com/epoc.php&quot;&gt;Emotiv Epoch&lt;/a&gt; which is pretty luxurious compared to any of the described solutions above. With 14 channels and electrodes that you can simply wet with contact solution (no gross, thick conductive gel in the hair), it’s pretty convenient, and I’ve seen good signals from it thus far. The drawback is that unless you buy the über-exensive “Research” edition, you won’t get access to raw signal data but only to an abstract Emotiv API that exposes things like “excitement” or “attention”. But if you’re lucky like me to have a university paying for your research license, it’s very nice.&lt;/p&gt;

        </content>
    </entry>
    
    <entry>
        <title>BeagleCache: A Network Accelerator for the Developing World</title>
        <link href="/BeagleCache/"/>
        <updated>2014-08-20T00:00:00-04:00</updated>
        <id>/BeagleCache</id>
        <author>
					<name></name>
					<uri>/</uri>
					
				</author>
        <content type="html">
        	
        	&lt;p&gt;Internet access in developing world countries is both extremely slow and expensive, due largely to a lack of physical infrastructure.  As a result, web caching—wherein a central web cache stores data fetched by users in the network and provides subsequent requesters with a cached copy of that data—can be effective in decreasing the lag users experience in these regions.  Paired with another strategy called “bandwidth shifting,” in which an accelerator node located in a high-bandwidth region compresses data for and manages the freshness for accelerator nodes in low-bandwidth regions, caching can be more effective still.  I was curious to know if these sophisticated acceleration techniques could run on a cheap, single-board ARM computer, which could ultimately provide a simple plug-and-play platform for developing world network accelerators.  In my research, I attempted to create such a platform on a BeagleBone Black (BBB), a $45 Linux computer no larger than a credit card.  After testing the BBB’s bandwidth in serving disk requests, I determined this cheap computer would have the chutzpa to serve as a web cache, and began developing caching/bandwidth-shifting software for it.  Using Node Javascript and C, I created a portable software stack that could run on the BBB, resulting in BeagleCache: A Low-Cost Caching Proxy for the Developing World.  &lt;/p&gt;

        </content>
    </entry>
    
    <entry>
        <title>Infrasonic Pixel Necklace</title>
        <link href="/pixel-necklace/"/>
        <updated>2014-07-28T00:00:00-04:00</updated>
        <id>/pixel-necklace</id>
        <author>
					<name></name>
					<uri>/</uri>
					
				</author>
        <content type="html">
        	
        	&lt;p&gt;This project was featured on the &lt;a href=&quot;http://www.adafruit.com/blog/2014/11/19/glowy-necklace-made-from-headphone-cable-wearablewednesday/&quot;&gt;Adafruit Wearables Blog&lt;/a&gt;, and was a Featured Article on &lt;a href=&quot;ttp://www.instructables.com/id/Electronics-Necklace-from-Headphone-Cable/?embed=flash&quot;&gt;Instructables&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;This pixel necklace was definitely my favorite project, mostly because it has quite a few design tricks up its sleeve.&lt;/p&gt;

&lt;p&gt;First, as far as connected necklaces go, it has a pretty small pendant with a powerful processor.  The trick is that the microcontroller controlling the tri-color LED pendant is actually hid behind my neck, and controls the LED via the super-thin wires in a headphone cable.  The clasp for the necklace is a headphone cable jack; I close the necklace by snapping the headphone cable jack into its socket, which also serves as a switch for the necklace.  You can read about how I worked with headphone cables in the &lt;a href=&quot;ttp://www.instructables.com/id/Electronics-Necklace-from-Headphone-Cable/?embed=flash&quot;&gt;Instructable&lt;/a&gt;.  &lt;/p&gt;

&lt;p&gt;My end goal in making this necklace was to have it alert me when my cellphone rang.  I didn’t want to deal with convincing my phone to send out a bluetooth signal when I received a call, though, so I decided to have the LED light up when my phone emitted a high-frequency, ultrasonic pulse.  This was a pretty cheap solution, since it meant I only had to have a microphone connected to my microcontroller.  I also gave my pixel necklace a mode in which it lit up in response to distinct freqeuncy progressions (i.e. like those in Seven Nation Army).  &lt;/p&gt;

&lt;p&gt;Below is some code to do this that I haven’t looked at for months, that was informed by &lt;a href=&quot;http://blog.theultimatelabs.com/2013/05/wirelessly-communicating-with-arduino.html&quot;&gt;this code&lt;/a&gt;, so I’m sorry if it’s nonsensical.&lt;br /&gt;
 //TODO: Make sense of this code&lt;/p&gt;

&lt;pre&gt;&lt;code&gt; #include &amp;lt;avr/pgmspace.h&amp;gt;
 #include &amp;lt;Adafruit_NeoPixel.h&amp;gt;

 boolean ready = 0;
 #define PIN 6
 int BUILTINPIN = 13;
 #define ADC_CENTER 127
 #define WINDOW_SIZE 128
 #define BIT_PERIOD 8
 #define BIT_PERIOD_2 16
 #define BIT_PERIOD_5_2 20
 #define BIT_PERIOD_3_2 12
 #define BIT_PERIOD_1_2 4
 #define MSG_LENGTH 3
 #define NUM_BYTES 5
 #define NUM_MSGS 5
 #define THRESHOLD 25000

 unsigned long mag;
 int counter = 0;
 const unsigned int ON_THRESHOLD = 500;
 const unsigned int OFF_THRESHOLD = ON_THRESHOLD/2;


 const float SAMPLE_RATE = 76923.0769;//38461.5385 //8928.57143
 const float STAMP_MS = (float)WINDOW_SIZE/(SAMPLE_RATE/1000);

 byte bytei = 0;
 byte biti = 0;
 byte lastSmpl = 0;
 byte bankIn = 0;
 byte bankProc = 0;

 unsigned short samplei=0;
 unsigned long stamp=0;
 unsigned long lastStamp=0;


 int Q1,Q2;
 //Adafruit_NeoPixel strip = Adafruit_NeoPixel(1, PIN, NEO_GRB + NEO_KHZ800);

 void setup() {

   // strip.begin();
   //strip.show(); // Initialize all pixels to &#39;off&#39;

   Serial.begin(9600);
   initADC();



 }

 int tcount = 0;
 void loop()
 {
   tcount++;
   Serial.print(&quot;Count is &quot;);
   Serial.println(counter);
   /*
   if(counter &amp;gt; 10)
   {
     rainbowCycle(5);
     // After, turn of neopixel
     strip.setPixelColor(0,0,0,0);
     strip.show();
   }
   */

 }

 // Slightly different, this makes the rainbow equally distributed throughout
 /*
 void rainbowCycle(uint8_t wait) {
   uint16_t i, j;

   for(j=0; j&amp;lt;256*5; j++) { // 5 cycles of all colors on wheel
     for(i=0; i&amp;lt; strip.numPixels(); i++) {
       strip.setPixelColor(i, Wheel(((i * 256 / strip.numPixels()) + j) &amp;amp; 255));
     }
     strip.show();
     delay(wait);
   }
 }
 */
 // Input a value 0 to 255 to get a color value.
 // The colours are a transition r - g - b - back to r.
 /*
 uint32_t Wheel(byte WheelPos) {
   if(WheelPos &amp;lt; 85) {
     return strip.Color(WheelPos * 3, 255 - WheelPos * 3, 0);
     } else if(WheelPos &amp;lt; 170) {
       WheelPos -= 85;
       return strip.Color(255 - WheelPos * 3, 0, WheelPos * 3);
       } else {
         WheelPos -= 170;
         return strip.Color(0, WheelPos * 3, 255 - WheelPos * 3);
       }
     }

     */
     void initADC() {
       cli();//diable interrupts

       //set up continuous sampling of analog pin 0

       //clear ADCSRA and ADCSRB registers
       ADCSRA = 0;
       ADCSRB = 0;

       ADMUX |= (1 &amp;lt;&amp;lt; REFS0); //set reference voltage
       ADMUX |= (1 &amp;lt;&amp;lt; ADLAR); //left align the ADC value- so we can read highest 8 bits from ADCH register only

       ADCSRA |= (1 &amp;lt;&amp;lt; ADPS2) ;//| (1 &amp;lt;&amp;lt; ADPS0); //set ADC clock with 32 prescaler- 16mHz/32=500kHz
       ADCSRA |= (1 &amp;lt;&amp;lt; ADATE); //enabble auto trigger
       enableADCInterrupt();//ADCSRA |= (1 &amp;lt;&amp;lt; ADIE); //enable interrupts when measurement complete
       ADCSRA |= (1 &amp;lt;&amp;lt; ADEN); //enable ADC
       ADCSRA |= (1 &amp;lt;&amp;lt; ADSC); //start ADC measurements

       sei();//enable interrupts
     }

     void enableADCInterrupt() {
       ADCSRA |= (1 &amp;lt;&amp;lt; ADIE); //enable interrupts when measurement complete
     }
     void disableADCInterrupt() {
       ADCSRA &amp;amp;= ~(1 &amp;lt;&amp;lt; ADIE); //enable interrupts when measurement complete
     }


     ISR(ADC_vect) {
       int Q0 = -Q2 + ((int)ADCH-ADC_CENTER);
       Q2 = Q1;
       Q1 = Q0;

       samplei++;
       if(samplei == WINDOW_SIZE) {
         samplei = 0;
         unsigned long mag = (unsigned long)((long)Q1)*Q1+(unsigned long)((long)Q2)*Q2;

         Q1=Q2=0;

         if(mag &amp;gt; ON_THRESHOLD) {
           counter = 1;
         }
         else if (mag &amp;lt; OFF_THRESHOLD) {
           counter = 0;
         }
         else
         {
           counter = 3;
         }
       }
     }
&lt;/code&gt;&lt;/pre&gt;

        </content>
    </entry>
    
    <entry>
        <title>SRON: A Software-Defined Overlay Network</title>
        <link href="/SRON/"/>
        <updated>2014-05-20T00:00:00-04:00</updated>
        <id>/SRON</id>
        <author>
					<name></name>
					<uri>/</uri>
					
				</author>
        <content type="html">
        	&lt;img src=&quot;/images/sron.png&quot;&gt;&lt;br/&gt;
        	&lt;p&gt;During the Spring semester of my junior year, I decided to do a research project
with Professor &lt;a href=&quot;http://www.cs.princeton.edu/~jrex/&quot;&gt;Jen Rexford&lt;/a&gt;, who is not only
a networking God but also probably the best advisor one can over hope to work with.
We worked together on a Software-Defined Networking project called
&lt;em&gt;SRON: A Software-Defined Overlay Network&lt;/em&gt;. I had never done research in
networking before, as usually I work on projects that involve working with
physical hardware devices.  Nonetheless, I learned a bunch from all of the obstacles
I faced in building SRON, from basic network programming to how to deal with
distributed computing problems like timing events across a distributed system.
And, not to brag, but I thought of some “neat” solutions to these problems
that you can read about in my &lt;a href=&quot;/files/SRON_whitepaper.pdf&quot;&gt;paper&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;So what is SRON? In short:&lt;/p&gt;

&lt;p&gt;SRON is software written in the network
programming language &lt;a href=&quot;http://www.frenetic-lang.org/pyretic/&quot;&gt;Pyretic&lt;/a&gt; that allows internet service
providers to create an &lt;a href=&quot;http://en.wikipedia.org/wiki/Overlay_network&quot;&gt;overlay network&lt;/a&gt; over their physical
network to help them better respond to routing outages, congestion, and more. In
particular, internet service providers (ISPs) can often monitor the routers within their
networks well, but once network traffic leaves their domain and is handed off
to a second ISP, the original ISP cannot easily monitor routing performance.
SRON allows a single ISP to monitor the latencies of various paths throughout
the Internet, even those paths that might cross through many ISP domains, and
to dynamically alter routing rules based on those latencies.  &lt;/p&gt;

        </content>
    </entry>
    
    <entry>
        <title>A Robot for Scanning Rooms in 3D</title>
        <link href="/Capturing-Pointcloud/"/>
        <updated>2013-08-20T00:00:00-04:00</updated>
        <id>/Capturing-Pointcloud</id>
        <author>
					<name></name>
					<uri>/</uri>
					
				</author>
        <content type="html">
        	&lt;img src=&quot;/images/point_cloud_feature.gif&quot;&gt;&lt;br/&gt;
        	&lt;p&gt;At &lt;a href=&quot;http://www.floored.com/&quot;&gt;Floored Inc&lt;/a&gt;, I built a device for taking 3D scans of interior spaces.  Using a Hokuyo laser rangefinder, a 2D color camera, and a computer mounted on a motor-controlled tripod, I was able to create some
nice point clouds (as above), linked with image data.  “Nelson,” as we called
v0 of this scanner, was built in Node.js (albeit with many C bindings).  Node’s asynchronous style was both
a blessing and a curse for this project; on the one hand, it made it easy to parallelize tasks that could
be performed independently of each other, but was a pain when work needed to be done sequentially.
I would love to share my code, but unfortunately it belongs to Floored! You can see a picture
of Nelson and me &lt;a href=&quot;/about&quot;&gt;here&lt;/a&gt;, though!&lt;/p&gt;

        </content>
    </entry>
    
</feed>